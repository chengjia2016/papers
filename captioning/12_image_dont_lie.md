# Images Don’t Lie: Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank

利用图片的视觉信息， 比单纯使用文字表示在搜索结果上提升很多。让原来通过文本模型认为相似的两个图片，但实际上视觉上相差深远，可以被区分开。  更主要的是, 把从图片中抽取的语意信息和原来基于文本抽取手工提炼的文本特征相结合一起使用，得到多模态排序模型。

论文最初提出两个问题:
1. 是否可以从图片中提取一些特征, 是在文字描述中不具备的?
2. 如何把图片特征这一新模态整合进现有的排序模型中?

数据是用1400万的Etsy商品图和列表。

训练一个CNN模型会受限于训练数据的量，尽管总体的数据量很多，但是对于一个query对应的样本量还是只有几百，尤其对于那些长尾的query更是少的可怜，这会导致训练一个CNN模型过拟合。迁移学习是一个常用的应对手法。

这里选择了VGG19，因其在1000类的图片分类上的优异表现。

2.1节讲了如何获得多模态的embedding
2.2节讲了如何把这个多模态的embedding整合到learn to rank model

listing包含：描述性的标题, 标签, listing_id, shop_id, 以及一个图片

多模态embedding的目标是: 把文字和图片各自模态下的向量表示用一个单一的向量表示出来。文字是一个稀疏的向量，而图片是一个稠密的向量。


BoW: bag-of-words, 维度包括了所有的词的个数+listing_id的个数+shop_id的个数。把文本表示成one-hot形式的bag-of-words向量  

把图像归一化成256像素，然后截取中间的224*224范围，输入到VGG网络中得到4096维的图像特征，把结果再经过L2正则得到最终的图像向量。

把图像向量和文字向量拼成一个向量后，维度是原来两个的总和。正样本是点击了的listing, 没有点击的随便拿出一个来构成负样本。然后根据生成的pairwise训练数据训练一个模型，当有query来时，把query和retrival回来的listing作为输入喂给模型，模型输出的分数越高则越匹配。
