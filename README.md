# papers
Some note when reading papers

## Image captioning

[0 note](https://github.com/wang-yang/papers/blob/master/captioning/0_note.md)  
[1 m-RNN](https://github.com/wang-yang/papers/blob/master/captioning/1_deep_captioning_with_m_rnn.md)  
[2 DCC-without-paired-training-data](https://github.com/wang-yang/papers/blob/master/captioning/2_deep_compositional_captioning_novel_object_without_paired_training_data.md)  
[3 unambiguous-object-descriptions](https://github.com/wang-yang/papers/blob/master/captioning/3_generation_and_comprehension_of_unambiguous_object_descriptions.md)  
[4 show-and-tell](https://github.com/wang-yang/papers/blob/master/captioning/4_show_and_tell.md)  
[5 show-attend-and-tell](https://github.com/wang-yang/papers/blob/master/captioning/5_show_attend_and_tell.md)  
[6 sequence-to-sequence-video-to-text](https://github.com/wang-yang/papers/blob/master/captioning/6_sequence_to_sequence_video_to_text.md)  
[7 deep-visual-semantic-alignments-for-generating-image-descriptions](https://github.com/wang-yang/papers/blob/master/captioning/7_deep_visual_semantic_alignments_for_generating_image_descriptions.md)  
[8 densecap](https://github.com/wang-yang/papers/blob/master/captioning/8_densecap_fully_convolutional_localization_networks_for_dense_captioning.md)  
[9 image-captioning-with-semantic-attention](https://github.com/wang-yang/papers/blob/master/captioning/9_image_captioning_with_semantic_attention.md)  

## Image QA

[stacked-attention-networks](https://github.com/wang-yang/papers/blob/master/qa/stacked_attention_networks_for_image_question_answering.md)  

## Object dectection

[r-cnn](https://github.com/wang-yang/papers/blob/master/object_dection/faster_rcnn_towards_realtime_object_dectection_with_region_proposal_networks.md)  
