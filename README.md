# papers
Some note when reading papers

## Notes

[Updated in my blog](wangyang.online/blog)  

[1 rnn-turorials](https://github.com/wang-yang/papers/blob/master/notes/1_rnn_tutorials.md)  
[2 origin-rnn](https://github.com/wang-yang/papers/blob/master/notes/2_origin_rnn.md)  
[3 lstm-gru](https://github.com/wang-yang/papers/blob/master/notes/3_lstm_gru.md)  
[4 char-rnn-numpy](https://github.com/wang-yang/papers/blob/master/notes/4_more_rnn.md)  
[5 neuraltalk-compile](https://github.com/wang-yang/papers/blob/master/notes/5_neuraltalk_compile.md)  
[6 neuraltalk-no-network](https://github.com/wang-yang/papers/blob/master/notes/6_neuraltalk_compile_without_network.md)  
[7 neuraltalk-docker](https://github.com/wang-yang/papers/blob/master/notes/7_docker_neuraltalks2.md)  
[8 image-caption-eval](https://github.com/wang-yang/papers/blob/master/notes/8_image_caption_eval.md)  
[9 neuraltalks2-code](https://github.com/wang-yang/papers/blob/master/notes/9_neuraltalks2_code.md)  
[10 neuraltalks2-web](https://github.com/wang-yang/papers/blob/master/notes/10_neuraltalks_web.md)  

## Image captioning

[0 note](https://github.com/wang-yang/papers/blob/master/captioning/0_note.md)  
[1 tf:m-RNN](https://github.com/wang-yang/papers/blob/master/captioning/1_deep_captioning_with_m_rnn.md)  
[2 caffe:DCC-without-paired-training-data](https://github.com/wang-yang/papers/blob/master/captioning/2_deep_compositional_captioning_novel_object_without_paired_training_data.md)  
[3 unambiguous-object-descriptions](https://github.com/wang-yang/papers/blob/master/captioning/3_generation_and_comprehension_of_unambiguous_object_descriptions.md)  
[4 tf:show-and-tell](https://github.com/wang-yang/papers/blob/master/captioning/4_show_and_tell.md)  
[5 tf:show-attend-and-tell](https://github.com/wang-yang/papers/blob/master/captioning/5_show_attend_and_tell.md)  
[6 sequence-to-sequence-video-to-text](https://github.com/wang-yang/papers/blob/master/captioning/6_sequence_to_sequence_video_to_text.md)  
[7 neuraltalk:deep-visual-semantic-alignments](https://github.com/wang-yang/papers/blob/master/captioning/7_deep_visual_semantic_alignments_for_generating_image_descriptions.md)  
[8 torch densecap](https://github.com/wang-yang/papers/blob/master/captioning/8_densecap_fully_convolutional_localization_networks_for_dense_captioning.md)  
[9 image-captioning-with-semantic-attention](https://github.com/wang-yang/papers/blob/master/captioning/9_image_captioning_with_semantic_attention.md)  

## Image QA

[stacked-attention-networks](https://github.com/wang-yang/papers/blob/master/qa/stacked_attention_networks_for_image_question_answering.md)  

## Object dectection

[r-cnn](https://github.com/wang-yang/papers/blob/master/object_dection/faster_rcnn_towards_realtime_object_dectection_with_region_proposal_networks.md)  
