# papers
Some note when reading papers

## Image captioning

[1 m-RNN](https://github.com/wang-yang/papers/blob/master/captioning/1_deep_captioning_with_m_rnn.md)

[2 DCC-without-paired-training-data](https://github.com/wang-yang/papers/blob/master/captioning/2_deep_compositional_captioning_novel_object_without_paired_training_data.md)

[3 unambiguous-object-descriptions](https://github.com/wang-yang/papers/blob/master/captioning/3_generation_and_comprehension_of_unambiguous_object_descriptions.md)

[4 show-and-tell](https://github.com/wang-yang/papers/blob/master/captioning/4_show_and_tell.md)

[5 show-attend-and-tell](https://github.com/wang-yang/papers/blob/master/captioning/5_show_attend_and_tell.md)

[6 sequence-to-sequence-video-to-text](https://github.com/wang-yang/papers/blob/master/captioning/6_sequence_to_sequence_video_to_text.md)

[7 deep-visual-semantic-alignments-for-generating-image-descriptions](https://github.com/wang-yang/papers/blob/master/captioning/7_deep_visual_semantic_alignments_for_generating_image_descriptions.md)

[8 densecap](https://github.com/wang-yang/papers/blob/master/captioning/8_densecap_fully_convolutional_localization_networks_for_dense_captioning.md)

## Image QA

[stacked-attention-networks](https://github.com/wang-yang/papers/blob/master/qa/stacked_attention_networks_for_image_question_answering.md)

## Object dectection

[r-cnn](https://github.com/wang-yang/papers/blob/master/object_dection/faster_rcnn_towards_realtime_object_dectection_with_region_proposal_networks.md)
